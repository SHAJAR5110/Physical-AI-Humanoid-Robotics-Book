# RAG Chatbot Implementation Plan

**Feature**: 002-rag-chatbot
**Plan Version**: 1.0
**Created**: 2025-12-08
**Status**: Architecture & Technical Design

---

## Executive Summary

This document provides the technical architecture, data model, API contracts, and deployment strategy for the RAG Chatbot feature. The plan translates feature requirements (spec.md) into a concrete, implementable design that adheres to the project's constitutional principles (code quality, performance, security, RAG quality).

---

## Table of Contents

1. [Technical Context & Architecture](#technical-context--architecture)
2. [Constitution Check](#constitution-check)
3. [Data Model](#data-model)
4. [API Contracts](#api-contracts)
5. [Backend Architecture](#backend-architecture)
6. [Frontend Integration](#frontend-integration)
7. [Deployment Strategy](#deployment-strategy)
8. [Monitoring & Observability](#monitoring--observability)
9. [Risk Mitigation](#risk-mitigation)
10. [Implementation Quickstart](#implementation-quickstart)

---

## Technical Context & Architecture

### Stack Decision Matrix

| Component | Technology | Why | Rationale |
|-----------|-----------|-----|-----------|
| **Backend API** | FastAPI (Python 3.11) | Fast, async, great docs | Native async/await; integrates with Claude SDK; PEP 8 friendly |
| **Frontend** | React + TypeScript | Type-safe, reactive | Docusaurus uses React; seamless component integration; supports TS for quality |
| **Embeddings** | Sentence-Transformers (local) | Free, no API key, full control | ~8GB on-disk; runs on CPU; achieves 0.85+ similarity for semantic matching |
| **Vector DB** | Qdrant Cloud (free tier) | Semantic search, free tier | 1M vectors, 100 MB, 50 requests/sec (free tier sufficient for MVP) |
| **LLM Provider** | Groq API (free tier) | Fast inference, free | 30,000 tokens/minute free tier; <100ms latency; ideal for MVP testing |
| **Database (optional)** | Neon Postgres | Serverless, free tier | For future conversation history; not needed for MVP (stateless chatbot) |
| **Deployment (backend)** | Render.com (free tier) | Python support, easy CI/CD | 0.5 CPU, 512MB RAM free tier; auto-redeploy on git push |
| **Deployment (frontend)** | Vercel | Docusaurus optimized, free | Edge caching; auto-deploy from git; integrates with book-source/ |
| **Async Task Queue** | Background tasks in FastAPI | Minimal overhead | Sufficient for MVP; process long-running tasks (cold start, bulk indexing) |

### Architecture Diagram

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     Docusaurus Frontend (All Pages)                ‚îÇ
‚îÇ                   book-source/src/theme/Root.tsx                   ‚îÇ
‚îÇ                                                                    ‚îÇ
‚îÇ  Chapter Content                   Floating ChatBot Widget         ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê             ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ Chapter Text     ‚îÇ             ‚îÇ üí¨ Toggle Button            ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ (ROS 2, Isaac, ..)          ‚îÇ                             ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò             ‚îÇ [Widget Opens on Click]      ‚îÇ  ‚îÇ
‚îÇ                                   ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ  ‚îÇ
‚îÇ  User selects text ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí   ‚îÇ ‚îÇ Ask about this Chapter  ‚îÇ ‚îÇ  ‚îÇ
‚îÇ  Widget auto-opens                ‚îÇ ‚îÇ                          ‚îÇ ‚îÇ  ‚îÇ
‚îÇ  with selected text               ‚îÇ ‚îÇ [Input + Submit Button]  ‚îÇ ‚îÇ  ‚îÇ
‚îÇ                                   ‚îÇ ‚îÇ [Response + Sources]     ‚îÇ ‚îÇ  ‚îÇ
‚îÇ                                   ‚îÇ ‚îÇ [Confidence + Feedback]  ‚îÇ ‚îÇ  ‚îÇ
‚îÇ                                   ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ  ‚îÇ
‚îÇ                                   ‚îÇ                             ‚îÇ  ‚îÇ
‚îÇ                                   ‚îÇ Fixed: bottom-left corner    ‚îÇ  ‚îÇ
‚îÇ                                   ‚îÇ Responsive on mobile (< 640px) ‚îÇ
‚îÇ                                   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                                                                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                  ‚îÇ HTTP POST /api/chat (JSON)
                                  ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    FastAPI Backend (Render)                  ‚îÇ
‚îÇ                   backend/main.py (FastAPI)                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ Endpoints:                                              ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  POST   /api/chat         - Process question            ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  GET    /health           - Health check                ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  GET    /api/docs         - API documentation           ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  ‚îÇ Services:                                                ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  - Embedding Service (Sentence-Transformers)            ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  - Retrieval Service (Qdrant client)                     ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  - Generation Service (Groq LLM)                         ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  - Confidence Calculator                                ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  - Error Handler & Formatter                            ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  ‚îÇ Models & Validation:                                     ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  - ChatRequest (Pydantic)                                ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  - ChatResponse (Pydantic)                               ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  - Document, Question, Answer (data classes)            ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îò
                              ‚îÇ                              ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                       ‚îÇ
                    ‚îÇ                ‚îÇ                       ‚îÇ
                    ‚Üì                ‚Üì                       ‚Üì
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ Sentence-Trans   ‚îÇ  ‚îÇ Qdrant Cloud ‚îÇ  ‚îÇ Groq API (LLM)     ‚îÇ
        ‚îÇ (Local Model)    ‚îÇ  ‚îÇ (Vector DB)  ‚îÇ  ‚îÇ (Answer Generation)‚îÇ
        ‚îÇ - Embed question ‚îÇ  ‚îÇ - Semantic   ‚îÇ  ‚îÇ - Synthesize       ‚îÇ
        ‚îÇ - Cache optim    ‚îÇ  ‚îÇ   search     ‚îÇ  ‚îÇ   answer from      ‚îÇ
        ‚îÇ                  ‚îÇ  ‚îÇ - 1M vectors ‚îÇ  ‚îÇ   context          ‚îÇ
        ‚îÇ                  ‚îÇ  ‚îÇ - 50 req/sec ‚îÇ  ‚îÇ                    ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Execution Flow: Question ‚Üí Answer

```
1. USER ASKS QUESTION
   ‚îú‚îÄ Input: Question text + optional selected text
   ‚îú‚îÄ UI shows loading spinner
   ‚îî‚îÄ Frontend sends HTTP POST /api/chat

2. EMBEDDING PHASE
   ‚îú‚îÄ Backend receives ChatRequest
   ‚îú‚îÄ Validate input (length < 1000 chars, no injection attacks)
   ‚îú‚îÄ Embed question using sentence-transformers (local)
   ‚îî‚îÄ Duration: ~100ms

3. RETRIEVAL PHASE
   ‚îú‚îÄ Query Qdrant with embedding + filters
   ‚îú‚îÄ Retrieve top-5 passages (ordered by similarity)
   ‚îú‚îÄ Apply relevance filter (cosine similarity ‚â• 0.85)
   ‚îú‚îÄ If no matches: return error to user
   ‚îî‚îÄ Duration: ~200ms

4. CONTEXT PREPARATION
   ‚îú‚îÄ Collect source metadata for each passage
   ‚îú‚îÄ Truncate to ‚â§ 6k tokens (context window limit)
   ‚îú‚îÄ Format as: "Module X.Y: [passage]\n---"
   ‚îî‚îÄ Duration: ~50ms

5. GENERATION PHASE
   ‚îú‚îÄ Call Groq API with prompt:
   ‚îÇ  "You are a tutor. Based on this context:\n[passages]\n
   ‚îÇ   Answer this question: [question]\n
   ‚îÇ   Keep answer concise (2-3 sentences)."
   ‚îú‚îÄ Receive LLM response
   ‚îî‚îÄ Duration: ~1500ms (variable based on answer length)

6. CONFIDENCE CALCULATION
   ‚îú‚îÄ Compute score from:
   ‚îÇ  - Max cosine similarity (40% weight)
   ‚îÇ  - Number of relevant passages (30% weight)
   ‚îÇ  - Key term matching (30% weight)
   ‚îú‚îÄ Return: high (‚â•0.8), medium (0.5-0.8), low (<0.5)
   ‚îî‚îÄ Duration: ~50ms

7. RESPONSE FORMATTING
   ‚îú‚îÄ Build ChatResponse JSON:
   ‚îÇ  - answer: LLM response
   ‚îÇ  - sources: [{chapter, module, section, excerpt}]
   ‚îÇ  - confidence: high|medium|low
   ‚îÇ  - processing_time_ms: total elapsed
   ‚îú‚îÄ Validate JSON schema
   ‚îî‚îÄ Duration: ~30ms

8. SEND TO USER
   ‚îú‚îÄ HTTP 200 response
   ‚îú‚îÄ Frontend receives JSON
   ‚îú‚îÄ Display: answer + sources + confidence
   ‚îú‚îÄ Hide loading spinner
   ‚îî‚îÄ Total time: 100+200+50+1500+50+30 = ~1930ms (typical, well under 3s p95)
```

---

## Constitution Check

### Mapping to Constitutional Principles

| Principle | Requirement | Plan Implementation | Status |
|-----------|-------------|-------------------|--------|
| **I. Content-First** | Accurate, pedagogically sound material | Answers synthesized from textbook content only (no hallucination); source attribution ensures traceability | ‚úÖ Met |
| **II. Reusability** | Modular skills & agents; no hardcoded logic | Embedding service (reusable), retrieval service (reusable), generation service (reusable); can be composed into tutoring agents | ‚úÖ Met |
| **III. Personalization** | User context in RAG responses | ChatRequest includes optional user_profile field (future); system prompt can adapt based on profile | ‚úÖ Met |
| **IV. Accessibility** | Mobile responsive, clear UX | React component mobile-first (320px+); error messages plain English; loading states visible | ‚úÖ Met |
| **V. Security & Privacy** | No API keys in logs; rate limiting | API keys in `.env`; Groq/Qdrant credentials never logged; rate limit middleware (10 req/min per IP) | ‚úÖ Met |
| **VI. RAG Chatbot Quality** | p95 ‚â§ 3s, ‚â•85% relevance, confidence scoring | Architecture ensures <3s latency; 0.85 similarity threshold; confidence calculated per FR-5; error handling graceful | ‚úÖ Met |

### Quality Gates

| Gate | Check | Status | Evidence |
|------|-------|--------|----------|
| **Code Quality** | Type hints on all functions | ‚úÖ Enforced | Pydantic models, function signatures with types, FastAPI auto-validation |
| **Testing** | ‚â•80% coverage target | ‚úÖ Built-in | FastAPI test utilities; pytest framework specified; coverage reports |
| **Performance** | p95 latency ‚â§ 3s | ‚úÖ Achievable | Async operations; local embeddings (no API latency); Groq <100ms LLM |
| **Security** | No secrets in logs | ‚úÖ Enforced | `.env` file usage; structured logging; secret redaction middleware |
| **Compliance** | WCAG 2.1 AA (for UI) | ‚úÖ Planned | React component with semantic HTML; ARIA labels; color contrast |

---

## Data Model

### Entities & Relationships

#### Document Entity (Vector DB Storage)

```
Document {
  id: UUID
  chapter: str           # "Chapter 2: ROS 2 Fundamentals"
  module: str            # "Module 2.1: Nodes and Topics"
  section: str           # "ros-2-topics" (anchor)
  text: str              # Full markdown text of passage
  tokens: int            # Token count (for context window mgmt)
  embedding: Vec[1536]   # Vector embedding (sentence-transformers)

  Constraints:
    - id is unique (primary key)
    - tokens computed at indexing time
    - embedding computed using sentence-transformers-all-minilm-l6-v2
}
```

**Storage**: Qdrant Cloud (free tier: 1M vectors max, 100 MB storage)

**Indexing Pipeline**:
1. Extract markdown chapters from `/docs/docs/Chapter-*.md`
2. Split by H3 sections (typical section = 300-400 tokens)
3. For each section: compute embedding, store in Qdrant
4. Metadata: chapter/module/section extracted from headings

---

#### Question Entity (Transient, for Logging)

```
Question {
  id: UUID
  text: str              # User's question ("What is ROS 2?")
  embedding: Vec[1536]   # Embedded question
  selected_text: str     # Optional selected passage from book
  timestamp: ISO8601     # When question was asked
  conversation_id: UUID  # Optional (for future multi-turn)

  Constraints:
    - id is unique (primary key)
    - text length < 1000 characters
    - selected_text is optional; if present, max 5000 chars
    - timestamp auto-generated on creation
    - conversation_id is null for MVP (stateless)
}
```

**Storage**: In-memory (not persisted for MVP; privacy-first)

**Lifecycle**:
- Created when user submits question
- Deleted after response is generated (unless user opts-in to logging)

---

#### Answer Entity (For Audit & Feedback)

```
Answer {
  id: UUID
  question_id: UUID      # Reference to Question
  text: str              # LLM-generated answer
  sources: [{            # List of source passages
    chapter: str         # Chapter name
    module: str          # Module name
    section: str         # Section anchor
    excerpt: str         # Relevant passage
  }]
  confidence: enum       # "high" | "medium" | "low"
  processing_time_ms: int # Latency (for perf monitoring)
  feedback: str          # Optional user feedback (thumbs up/down)

  Constraints:
    - id is unique (primary key)
    - sources list has 1-5 items
    - confidence derived from similarity scores (deterministic)
    - processing_time_ms is non-negative integer
    - feedback is null for MVP (no storage)
}
```

**Storage**: In-memory (not persisted for MVP; stateless chatbot)

**Lifecycle**:
- Created when answer is generated
- Returned to frontend in ChatResponse
- Discarded after response sent (unless user opts-in to feedback)

---

### Relationships

```
User (Frontend)
  ‚Üì asks question
Question
  ‚Üì retrieves passages
Document (Qdrant)
  ‚Üì synthesizes
Answer
  ‚Üì returns to
User (Frontend)
```

**No direct user persistence for MVP** (privacy-first; no accounts/sessions)

---

## API Contracts

### Chat Endpoint: POST /api/chat

**Purpose**: Accept a question and return an answer with sources

**Request**:

```json
{
  "question": "What is ROS 2?",
  "selected_text": "Optional selected passage from book"
}
```

**Schema Validation** (Pydantic):

```python
class ChatRequest(BaseModel):
    question: str = Field(..., min_length=3, max_length=1000)
    selected_text: Optional[str] = Field(None, max_length=5000)

    @validator('question')
    def validate_question(cls, v):
        # Prevent injection attacks
        if any(c in v for c in ['<script>', 'DROP', 'DELETE']):
            raise ValueError('Invalid characters')
        return v
```

**Response** (Success - 200):

```json
{
  "answer": "ROS 2 is a middleware for robotics...",
  "sources": [
    {
      "chapter": "Chapter 2: ROS 2 Fundamentals",
      "module": "Module 2.1: Nodes and Topics",
      "section": "ros-2-topics",
      "excerpt": "ROS 2 is built on a publish-subscribe architecture..."
    }
  ],
  "confidence": "high",
  "processing_time_ms": 1850
}
```

**Schema Validation** (Pydantic):

```python
class SourceRef(BaseModel):
    chapter: str
    module: str
    section: str
    excerpt: str

class ChatResponse(BaseModel):
    answer: str = Field(..., min_length=10, max_length=2000)
    sources: List[SourceRef] = Field(..., min_items=1, max_items=5)
    confidence: Literal["high", "medium", "low"]
    processing_time_ms: int = Field(..., ge=0)
```

**Response** (Error - 400 Bad Request):

```json
{
  "error": "Validation failed",
  "details": "question must be at least 3 characters"
}
```

**Response** (Error - 503 Service Unavailable):

```json
{
  "error": "Chat service is temporarily unavailable",
  "details": "Vector database is unreachable. Please try again in a moment."
}
```

**Response** (Error - 429 Too Many Requests):

```json
{
  "error": "Rate limit exceeded",
  "details": "You have exceeded 10 requests per minute. Please wait before asking another question."
}
```

### Health Check Endpoint: GET /health

**Purpose**: Verify backend is running and dependencies are healthy

**Response** (200):

```json
{
  "status": "healthy",
  "timestamp": "2025-12-08T23:30:00Z",
  "dependencies": {
    "sentence_transformers": "loaded",
    "qdrant": "connected",
    "groq_api": "available"
  }
}
```

**Response** (503):

```json
{
  "status": "unhealthy",
  "dependencies": {
    "qdrant": "disconnected",
    "groq_api": "unavailable"
  }
}
```

### API Documentation

- **Format**: OpenAPI 3.0 (auto-generated by FastAPI)
- **Endpoint**: `GET /api/docs` (Swagger UI)
- **Alternative**: `GET /api/redoc` (ReDoc)

---

## Backend Architecture

### Project Structure

```
backend/
‚îú‚îÄ‚îÄ main.py                          # FastAPI app, routing
‚îú‚îÄ‚îÄ config.py                        # Configuration (env vars, constants)
‚îú‚îÄ‚îÄ requirements.txt                 # Dependencies
‚îú‚îÄ‚îÄ .env.example                     # Environment template
‚îú‚îÄ‚îÄ render.yaml                      # Render.com deployment config
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ models.py                    # Pydantic models (ChatRequest, ChatResponse)
‚îÇ   ‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ embedding_service.py     # Sentence-Transformers embedding
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ retrieval_service.py     # Qdrant vector search
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ generation_service.py    # Groq LLM API calls
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ confidence_service.py    # Confidence scoring
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ error_handler.py         # Formatted error responses
‚îÇ   ‚îú‚îÄ‚îÄ middleware/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ cors.py                  # CORS configuration
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ rate_limit.py            # Rate limiting (10 req/min)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ logging.py               # Structured logging, secret redaction
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ exception_handler.py     # Global exception handling
‚îÇ   ‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ context_manager.py       # Token limit management
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ validators.py            # Input validation
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ response_formatter.py    # JSON response formatting
‚îÇ   ‚îî‚îÄ‚îÄ routers/
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îú‚îÄ‚îÄ chat.py                  # POST /api/chat endpoint
‚îÇ       ‚îî‚îÄ‚îÄ health.py                # GET /health endpoint
‚îî‚îÄ‚îÄ tests/
    ‚îú‚îÄ‚îÄ __init__.py
    ‚îú‚îÄ‚îÄ conftest.py                  # Pytest fixtures
    ‚îú‚îÄ‚îÄ test_embedding_service.py    # Unit tests
    ‚îú‚îÄ‚îÄ test_retrieval_service.py
    ‚îú‚îÄ‚îÄ test_generation_service.py
    ‚îú‚îÄ‚îÄ test_confidence_service.py
    ‚îú‚îÄ‚îÄ test_models.py
    ‚îú‚îÄ‚îÄ test_validators.py
    ‚îú‚îÄ‚îÄ test_endpoints.py            # Integration tests
    ‚îî‚îÄ‚îÄ test_e2e.py                  # End-to-end tests
```

### Key Services

#### 1. Embedding Service

```python
# src/services/embedding_service.py

class EmbeddingService:
    def __init__(self):
        # Load model once at startup
        self.model = SentenceTransformer(
            'all-minilm-l6-v2',
            device='cpu'  # or 'cuda' if available
        )

    def embed_text(self, text: str) -> Vec[1536]:
        """
        Embed text using sentence-transformers.
        Returns 1536-dimensional vector.
        """
        # Normalize text
        text = text.strip()
        if not text:
            raise ValueError("Empty text")

        # Generate embedding
        embedding = self.model.encode(
            text,
            convert_to_numpy=True,
            show_progress_bar=False
        )

        return embedding.tolist()  # Return as list[float]

    def get_model_info(self) -> dict:
        """Return model metadata for debugging."""
        return {
            "model": "all-minilm-l6-v2",
            "dimensions": 1536,
            "device": "cpu"
        }
```

**Performance**:
- Model size: ~90 MB
- Inference time: ~50-100ms per text
- Memory: ~500 MB on load
- Batch processing: Supported for bulk indexing

---

#### 2. Retrieval Service

```python
# src/services/retrieval_service.py

class RetrievalService:
    def __init__(self, qdrant_url: str, api_key: str):
        self.client = QdrantClient(
            url=qdrant_url,
            api_key=api_key
        )
        self.collection_name = "book_passages"

    def search(
        self,
        embedding: Vec[1536],
        top_k: int = 5,
        similarity_threshold: float = 0.85
    ) -> List[SourceRef]:
        """
        Search Qdrant for similar passages.
        Apply similarity threshold before returning.
        """
        results = self.client.search(
            collection_name=self.collection_name,
            query_vector=embedding,
            limit=top_k
        )

        # Filter by threshold
        filtered = [
            result for result in results
            if result.score >= similarity_threshold
        ]

        if not filtered:
            raise RetrievalError(
                "No relevant content found. "
                "Try rephrasing your question."
            )

        # Extract metadata and return
        return [
            SourceRef(
                chapter=result.payload['chapter'],
                module=result.payload['module'],
                section=result.payload['section'],
                excerpt=result.payload['text']
            )
            for result in filtered
        ]

    def health_check(self) -> bool:
        """Verify Qdrant connection."""
        try:
            self.client.get_collections()
            return True
        except Exception:
            return False
```

**Configuration**:
- Qdrant URL: `https://<your-cluster>.qdrant.io` (free tier)
- API Key: from Qdrant Cloud dashboard
- Collection: "book_passages" (created on first indexing)

---

#### 3. Generation Service

```python
# src/services/generation_service.py

class GenerationService:
    def __init__(self, groq_api_key: str):
        self.client = Groq(api_key=groq_api_key)
        self.model = "mixtral-8x7b-32768"  # Fast, free tier

    def generate_answer(
        self,
        question: str,
        context: str,  # Formatted passages (‚â§ 6k tokens)
        max_tokens: int = 512
    ) -> str:
        """
        Call Groq LLM to synthesize answer from context.
        """
        system_prompt = (
            "You are a knowledgeable tutor for a Physical AI textbook. "
            "Answer student questions based ONLY on the provided context. "
            "Keep answers concise (2-3 sentences). "
            "If context is insufficient, say 'I don't have enough information'."
        )

        user_message = (
            f"Context:\n{context}\n\n"
            f"Question: {question}\n\n"
            f"Answer:"
        )

        response = self.client.chat.completions.create(
            model=self.model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_message}
            ],
            max_tokens=max_tokens,
            temperature=0.3  # Lower temp for factual answers
        )

        return response.choices[0].message.content

    def health_check(self) -> bool:
        """Verify Groq API connectivity."""
        try:
            self.client.models.list()
            return True
        except Exception:
            return False
```

**Configuration**:
- API Key: from Groq console (free tier: 30,000 tokens/min)
- Model: `mixtral-8x7b-32768` (fast, accurate, free)
- Temperature: 0.3 (factual, not creative)

---

#### 4. Confidence Service

```python
# src/services/confidence_service.py

class ConfidenceService:
    def calculate_confidence(
        self,
        sources: List[SourceRef],
        answer: str,
        question: str
    ) -> Literal["high", "medium", "low"]:
        """
        Calculate confidence based on:
        1. Max similarity (40% weight)
        2. Number of relevant passages (30% weight)
        3. Key term matching (30% weight)
        """
        # Extract similarity from first source
        max_similarity = self._extract_similarity(sources[0])

        # Weight 1: Max similarity (0-1 scale)
        sim_score = max_similarity * 0.4

        # Weight 2: Number of sources (1-5 scale)
        num_sources = min(len(sources), 5)
        source_score = (num_sources / 5) * 0.3

        # Weight 3: Key terms from question present in answer
        question_terms = set(question.lower().split())
        answer_lower = answer.lower()
        matching_terms = sum(
            1 for term in question_terms
            if term in answer_lower
        )
        term_score = min(matching_terms / max(len(question_terms), 1), 1.0) * 0.3

        total_score = sim_score + source_score + term_score

        # Map to confidence level
        if total_score >= 0.8:
            return "high"
        elif total_score >= 0.5:
            return "medium"
        else:
            return "low"

    def _extract_similarity(self, source: SourceRef) -> float:
        # Note: Similarity stored in source metadata
        # This is a placeholder; actual implementation depends on
        # how similarity is passed through Retrieval Service
        return getattr(source, 'similarity', 0.9)
```

---

### Main FastAPI Application

```python
# backend/main.py

from fastapi import FastAPI, Depends
from fastapi.middleware.cors import CORSMiddleware
from src.routers import chat, health
from src.middleware import rate_limit, logging as logging_middleware
from src.services import (
    EmbeddingService,
    RetrievalService,
    GenerationService,
    ConfidenceService
)

# Initialize app
app = FastAPI(
    title="RAG Chatbot API",
    description="Q&A chatbot for Physical AI textbook",
    version="1.0.0"
)

# Load environment variables
from src.config import (
    QDRANT_URL,
    QDRANT_API_KEY,
    GROQ_API_KEY,
    ALLOWED_ORIGINS
)

# Initialize services (singleton pattern)
embedding_service = EmbeddingService()
retrieval_service = RetrievalService(QDRANT_URL, QDRANT_API_KEY)
generation_service = GenerationService(GROQ_API_KEY)
confidence_service = ConfidenceService()

# Middleware
app.add_middleware(logging_middleware.LoggingMiddleware)
app.add_middleware(rate_limit.RateLimitMiddleware)
app.add_middleware(
    CORSMiddleware,
    allow_origins=ALLOWED_ORIGINS,
    allow_credentials=True,
    allow_methods=["POST", "GET"],
    allow_headers=["Content-Type"]
)

# Routers
app.include_router(chat.router)
app.include_router(health.router)

@app.on_event("startup")
async def startup():
    """Health checks on startup."""
    print("Backend started. Checking dependencies...")
    if not embedding_service.get_model_info():
        raise RuntimeError("Failed to load embedding model")
    if not retrieval_service.health_check():
        print("Warning: Qdrant unavailable (will retry on request)")
    if not generation_service.health_check():
        print("Warning: Groq API unavailable (will retry on request)")

@app.on_event("shutdown")
async def shutdown():
    """Cleanup on shutdown."""
    print("Backend shutting down.")
```

---

## Frontend Integration

### Architecture: Floating Widget vs. Dedicated Page

**CRITICAL UPDATE**: The chatbot is **NOT** a dedicated page. Instead, it is a **floating widget** that appears in the bottom-left corner of all book pages (docs). This allows students to ask questions about any chapter they're reading without navigating away.

**Key Implications**:
1. **Layout Integration**: Widget is rendered globally via Docusaurus theme wrapping, not a page route
2. **State Management**: Ephemeral; cleared when user navigates between chapters
3. **Styling**: Floating box with fixed positioning; responsive on mobile (smaller on small screens)
4. **Accessibility**: Draggable (optional), closeable, non-intrusive

### React Component: ChatBot.tsx (Floating Widget)

**Location**: `book-source/src/components/ChatBot.tsx`

```typescript
import React, { useState, useRef } from 'react';
import styles from './ChatBot.module.css';

interface Source {
  chapter: string;
  module: string;
  section: string;
  excerpt: string;
}

interface ChatResponse {
  answer: string;
  sources: Source[];
  confidence: 'high' | 'medium' | 'low';
  processing_time_ms: number;
}

const ChatBot: React.FC = () => {
  const [isOpen, setIsOpen] = useState(false);
  const [question, setQuestion] = useState('');
  const [selectedText, setSelectedText] = useState('');
  const [response, setResponse] = useState<ChatResponse | null>(null);
  const [loading, setLoading] = useState(false);
  const [error, setError] = useState('');
  const [estimatedTime, setEstimatedTime] = useState<number | null>(null);
  const timeoutRef = useRef<NodeJS.Timeout>();

  const handleAskQuestion = async (e: React.FormEvent) => {
    e.preventDefault();

    if (!question.trim()) {
      setError('Please enter a question');
      return;
    }

    setLoading(true);
    setError('');
    setResponse(null);
    setEstimatedTime(null);

    // Show estimated time after 2 seconds
    const timeoutId = setTimeout(() => {
      setEstimatedTime(Date.now());
    }, 2000);

    try {
      const res = await fetch('/api/chat', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          question: question.trim(),
          selected_text: selectedText || undefined
        })
      });

      clearTimeout(timeoutId);

      if (!res.ok) {
        const errorData = await res.json();
        throw new Error(errorData.details || 'Failed to get response');
      }

      const data: ChatResponse = await res.json();
      setResponse(data);
    } catch (err) {
      setError(err instanceof Error ? err.message : 'An error occurred');
    } finally {
      setLoading(false);
      setEstimatedTime(null);
    }
  };

  const handleTextSelection = () => {
    const selected = window.getSelection()?.toString() || '';
    if (selected && selected.length > 10) {
      setSelectedText(selected);
      setIsOpen(true);  // Auto-open on selection
    }
  };

  const handleFeedback = (thumbs: 'up' | 'down') => {
    // Log feedback (future integration)
    console.log(`User gave ${thumbs} feedback`);
  };

  const handleClose = () => {
    setIsOpen(false);
    // Optionally clear state on close
    // setQuestion('');
    // setResponse(null);
  };

  const handleClearConversation = () => {
    setQuestion('');
    setSelectedText('');
    setResponse(null);
    setError('');
  };

  return (
    <>
      {/* Toggle Button - Always Visible */}
      <button
        className={styles.toggleButton}
        onClick={() => setIsOpen(!isOpen)}
        title="Ask about this chapter"
        aria-label="Open chatbot"
      >
        üí¨
      </button>

      {/* Floating Widget - Shows when Open */}
      {isOpen && (
        <div className={styles.chatWidget}>
          {/* Header with Close Button */}
          <div className={styles.widgetHeader}>
            <div className={styles.headerTitle}>
              <h3>Ask about this Chapter</h3>
            </div>
            <button
              onClick={handleClose}
              className={styles.closeButton}
              aria-label="Close chatbot"
            >
              ‚úï
            </button>
          </div>

          {/* Input Section */}
          <form onSubmit={handleAskQuestion} className={styles.inputSection}>
            <textarea
              value={question}
              onChange={(e) => setQuestion(e.target.value)}
              placeholder="E.g., What is ROS 2?"
              disabled={loading}
              className={styles.questionInput}
              rows={2}
            />

            {selectedText && (
              <div className={styles.selectedTextNotice}>
                üìå "{selectedText.substring(0, 40)}..."
                <button
                  type="button"
                  onClick={() => setSelectedText('')}
                  className={styles.clearSelection}
                >
                  ‚úï
                </button>
              </div>
            )}

            <button
              type="submit"
              disabled={loading}
              className={styles.submitButton}
            >
              {loading ? '...' : 'Ask'}
            </button>
          </form>

          {/* Loading State */}
          {loading && (
            <div className={styles.loadingState}>
              <div className={styles.spinner}></div>
              {estimatedTime && (
                <p className={styles.estimatedTime}>Still loading...</p>
              )}
            </div>
          )}

          {/* Error State */}
          {error && !loading && (
            <div className={styles.errorMessage}>
              <p>‚ùå {error}</p>
              <button onClick={() => setError('')} className={styles.dismissBtn}>
                Dismiss
              </button>
            </div>
          )}

          {/* Response Section */}
          {response && !loading && (
            <div className={styles.responseSection}>
              <div className={styles.answerContainer}>
                <p className={styles.answer}>{response.answer}</p>

                <div className={styles.metadata}>
                  <span className={`${styles.confidence} ${styles[response.confidence]}`}>
                    {response.confidence === 'high' && '‚úÖ'}
                    {response.confidence === 'medium' && '‚ö†Ô∏è'}
                    {response.confidence === 'low' && '‚ùì'}
                  </span>
                </div>
              </div>

              <div className={styles.sourcesSection}>
                <p className={styles.sourcesLabel}>Sources:</p>
                {response.sources.map((source, idx) => (
                  <a
                    key={idx}
                    href={`/docs/${source.section}`}
                    className={styles.sourceLink}
                    target="_blank"
                    rel="noopener noreferrer"
                  >
                    {source.module}
                  </a>
                ))}
              </div>

              <div className={styles.feedbackSection}>
                <button
                  onClick={() => handleFeedback('up')}
                  className={styles.feedbackButton}
                  title="This answer was helpful"
                >
                  üëç
                </button>
                <button
                  onClick={() => handleFeedback('down')}
                  className={styles.feedbackButton}
                  title="This answer was not helpful"
                >
                  üëé
                </button>
                <button
                  onClick={handleClearConversation}
                  className={styles.clearButton}
                  title="Clear conversation"
                >
                  üîÑ
                </button>
              </div>
            </div>
          )}
        </div>
      )}
    </>
  );
};

export default ChatBot;
```

### CSS Module: ChatBot.module.css

**Design**: Floating widget in bottom-left corner with smooth animations and mobile responsiveness.

```css
/* Toggle Button - Always Visible */
.toggleButton {
  position: fixed;
  bottom: 20px;
  left: 20px;
  width: 56px;
  height: 56px;
  border-radius: 50%;
  background: linear-gradient(135deg, #0ea5e9 0%, #8b5cf6 100%);
  border: none;
  color: white;
  font-size: 28px;
  cursor: pointer;
  box-shadow: 0 4px 12px rgba(14, 165, 233, 0.4);
  display: flex;
  align-items: center;
  justify-content: center;
  z-index: 999;
  transition: all 0.3s ease;
}

.toggleButton:hover {
  transform: scale(1.1);
  box-shadow: 0 6px 16px rgba(14, 165, 233, 0.6);
}

.toggleButton:active {
  transform: scale(0.95);
}

/* Floating Widget Container */
.chatWidget {
  position: fixed;
  bottom: 90px;
  left: 20px;
  width: 380px;
  max-height: 600px;
  background: white;
  border-radius: 12px;
  box-shadow: 0 5px 40px rgba(0, 0, 0, 0.16);
  display: flex;
  flex-direction: column;
  z-index: 998;
  animation: slideUp 0.3s ease-out;
  overflow: hidden;
}

@keyframes slideUp {
  from {
    opacity: 0;
    transform: translateY(20px);
  }
  to {
    opacity: 1;
    transform: translateY(0);
  }
}

/* Widget Header */
.widgetHeader {
  display: flex;
  justify-content: space-between;
  align-items: center;
  padding: 16px;
  background: linear-gradient(135deg, #0ea5e9 0%, #8b5cf6 100%);
  color: white;
  border-bottom: 1px solid rgba(0, 0, 0, 0.1);
}

.headerTitle h3 {
  margin: 0;
  font-size: 16px;
  font-weight: 600;
}

.closeButton {
  background: rgba(255, 255, 255, 0.2);
  border: none;
  color: white;
  font-size: 20px;
  cursor: pointer;
  padding: 4px 8px;
  border-radius: 4px;
  transition: background 0.2s;
}

.closeButton:hover {
  background: rgba(255, 255, 255, 0.3);
}

/* Input Section */
.inputSection {
  display: flex;
  flex-direction: column;
  gap: 8px;
  padding: 12px;
  border-bottom: 1px solid #e0e0e0;
}

.questionInput {
  width: 100%;
  padding: 8px;
  font-size: 14px;
  border: 1px solid #ddd;
  border-radius: 4px;
  font-family: inherit;
  resize: none;
  min-height: 50px;
  max-height: 100px;
}

.questionInput:focus {
  outline: none;
  border-color: #0ea5e9;
  box-shadow: 0 0 0 3px rgba(14, 165, 233, 0.1);
}

.questionInput:disabled {
  background: #f5f5f5;
  cursor: not-allowed;
}

.selectedTextNotice {
  display: flex;
  justify-content: space-between;
  align-items: center;
  padding: 8px;
  background: #e3f2fd;
  border-left: 3px solid #0ea5e9;
  border-radius: 4px;
  font-size: 12px;
  color: #0c5aa0;
  gap: 8px;
}

.clearSelection {
  background: none;
  border: none;
  color: #0c5aa0;
  cursor: pointer;
  font-size: 16px;
  padding: 0;
  margin-left: auto;
  flex-shrink: 0;
}

.clearSelection:hover {
  opacity: 0.7;
}

.submitButton {
  padding: 8px;
  background: linear-gradient(135deg, #0ea5e9 0%, #8b5cf6 100%);
  color: white;
  border: none;
  border-radius: 4px;
  cursor: pointer;
  font-size: 14px;
  font-weight: 600;
  transition: all 0.3s;
}

.submitButton:hover:not(:disabled) {
  transform: translateY(-2px);
  box-shadow: 0 2px 8px rgba(14, 165, 233, 0.3);
}

.submitButton:disabled {
  background: #ccc;
  cursor: not-allowed;
}

.loadingState {
  display: flex;
  flex-direction: column;
  align-items: center;
  justify-content: center;
  padding: 20px;
  min-height: 80px;
}

.spinner {
  width: 32px;
  height: 32px;
  border: 3px solid #f3f3f3;
  border-top: 3px solid #0ea5e9;
  border-radius: 50%;
  animation: spin 1s linear infinite;
  margin-bottom: 8px;
}

@keyframes spin {
  0% { transform: rotate(0deg); }
  100% { transform: rotate(360deg); }
}

.estimatedTime {
  font-size: 12px;
  color: #ff9800;
  margin-top: 4px;
}

.errorMessage {
  padding: 10px;
  background: #ffebee;
  border-left: 3px solid #f44336;
  border-radius: 4px;
  color: #c62828;
  font-size: 13px;
  margin: 12px 12px 0;
}

.errorMessage p {
  margin: 0 0 8px 0;
}

.dismissBtn {
  padding: 4px 12px;
  background: #f44336;
  color: white;
  border: none;
  border-radius: 3px;
  cursor: pointer;
  font-size: 12px;
  transition: background 0.2s;
}

.dismissBtn:hover {
  background: #d32f2f;
}

.responseSection {
  display: flex;
  flex-direction: column;
  gap: 10px;
  padding: 12px;
  overflow-y: auto;
  max-height: 300px;
}

.answerContainer {
  margin: 0;
}

.answer {
  font-size: 14px;
  line-height: 1.5;
  color: #333;
  margin: 0;
}

.metadata {
  display: flex;
  gap: 8px;
  font-size: 12px;
  color: #666;
  align-items: center;
  margin-top: 6px;
}

.confidence {
  font-size: 16px;
}

.confidence.high {
  opacity: 1;
}

.confidence.medium {
  opacity: 0.8;
}

.confidence.low {
  opacity: 0.6;
}

.sourcesSection {
  margin: 0;
}

.sourcesLabel {
  margin: 0 0 6px 0;
  font-size: 12px;
  font-weight: 600;
  color: #666;
  text-transform: uppercase;
}

.sourceLink {
  display: block;
  color: #0ea5e9;
  text-decoration: none;
  font-weight: 500;
  font-size: 13px;
  padding: 4px 6px;
  border-left: 2px solid #0ea5e9;
  border-radius: 2px;
  transition: all 0.2s;
  margin-bottom: 4px;
}

.sourceLink:hover {
  color: #8b5cf6;
  border-left-color: #8b5cf6;
  padding-left: 8px;
}

.sourceLink:last-child {
  margin-bottom: 0;
}

.feedbackSection {
  display: flex;
  gap: 6px;
  justify-content: flex-start;
  padding-top: 8px;
  border-top: 1px solid #e0e0e0;
  margin-top: 8px;
}

.feedbackButton,
.clearButton {
  background: #f5f5f5;
  border: 1px solid #ddd;
  border-radius: 4px;
  cursor: pointer;
  padding: 4px 8px;
  font-size: 16px;
  transition: all 0.2s;
  flex: 1;
}

.feedbackButton:hover,
.clearButton:hover {
  background: #e0e0e0;
  border-color: #999;
}

.clearButton {
  flex: 0 0 auto;
  padding: 4px 6px;
}

/* Mobile Responsiveness */
@media (max-width: 640px) {
  .chatWidget {
    width: calc(100vw - 40px);
    max-width: 360px;
    max-height: 80vh;
    bottom: 80px;
  }

  .questionInput {
    font-size: 16px; /* Prevent iOS zoom */
  }

  .responseSection {
    max-height: 250px;
  }

  .metadata {
    flex-direction: column;
    align-items: flex-start;
  }

  .sourceLink {
    font-size: 12px;
  }
}

@media (max-height: 600px) {
  .chatWidget {
    max-height: 70vh;
  }

  .responseSection {
    max-height: 150px;
  }
}
```

### Integration in Docusaurus (Global Widget)

**Architecture**: The chatbot widget is rendered globally on all doc pages using a custom Docusaurus wrapper component. This approach ensures the widget appears on every chapter page without requiring manual inclusion.

#### Step 1: Create Root Wrapper Component

**Location**: `book-source/src/theme/Root.tsx`

```typescript
import React from 'react';
import ChatBot from '../components/ChatBot';

type RootProps = {
  children: React.ReactNode;
};

export default function Root({ children }: RootProps): JSX.Element {
  return (
    <>
      {children}
      <ChatBot />
    </>
  );
}
```

This component wraps the entire Docusaurus app. The `<ChatBot />` widget renders globally at the bottom-left, appearing on all pages (docs, homepage, etc.).

#### Step 2: Register Root Wrapper in Docusaurus Config

**Location**: `book-source/docusaurus.config.js`

```javascript
const config = {
  // ... other config
  themes: [
    [
      '@docusaurus/preset-classic',
      {
        theme: {
          customCss: require.resolve('./src/css/custom.css'),
        },
      },
    ],
  ],
  // Add this to swizzle the root component
  swizzle: [
    {
      component: 'Root',
      method: 'wrapper',
      isTs: true,
    },
  ],
};
```

Alternatively, if Docusaurus version supports it directly:

```javascript
const config = {
  // ... other config
  // Path to your custom Root component
  themes: [
    {
      name: '@docusaurus/preset-classic',
      options: {
        theme: {
          customCss: require.resolve('./src/css/custom.css'),
        },
      },
    },
  ],
};

// Add custom theme wrappers
config.customFields = config.customFields || {};
config.customFields.rootComponent = './src/theme/Root.tsx';
```

**Note**: If your Docusaurus version doesn't support the above, use `swizzle` to override the layout:

```bash
cd book-source
npm run swizzle @docusaurus/preset-classic Layout -- --eject
```

Then edit the swizzled `Layout.tsx` to include the `<ChatBot />` component.

#### Result

- ‚úÖ Chatbot appears as a floating button (üí¨) in the bottom-left corner on all doc pages
- ‚úÖ Clicking button opens the widget
- ‚úÖ Widget is sticky (stays in bottom-left as user scrolls)
- ‚úÖ No layout shifts or disruptions to book content
- ‚úÖ Mobile-responsive: scales down on small screens

---

## Deployment Strategy

### Backend Deployment (Render.com)

#### Step 1: Create Render Service

1. Connect GitHub repository to Render
2. Select `book-source` branch (or create new branch for backend)
3. Set build command: `pip install -r requirements.txt`
4. Set start command: `uvicorn backend.main:app --host 0.0.0.0 --port $PORT`

#### Step 2: Environment Variables

Set these in Render dashboard:

```
QDRANT_URL=https://<cluster>.qdrant.io
QDRANT_API_KEY=<your-api-key>
GROQ_API_KEY=<your-api-key>
ALLOWED_ORIGINS=https://book.yoursite.com,http://localhost:3000
ENVIRONMENT=production
LOG_LEVEL=info
```

#### Step 3: Render Configuration File

**File**: `backend/render.yaml`

```yaml
services:
  - type: web
    name: rag-chatbot-api
    env: python
    plan: free
    buildCommand: pip install -r requirements.txt
    startCommand: uvicorn backend.main:app --host 0.0.0.0 --port $PORT
    envVars:
      - key: PYTHON_VERSION
        value: 3.11
      - fromGroup: chat-bot-env
```

### Frontend Deployment (Vercel)

#### Step 1: Configure Vercel

1. Connect `book-source` repository to Vercel
2. Set root directory: `book-source`
3. Build command: `npm run build`
4. Output directory: `.docusaurus/build`

#### Step 2: Environment Variables

```
REACT_APP_API_BASE_URL=https://rag-chatbot-api.onrender.com
```

#### Step 3: Deployment

- Automatic deployment on push to `master`
- Preview deployments on pull requests

### Data Indexing Pipeline

#### Initial Indexing (One-time)

**Script**: `backend/scripts/index_chapters.py`

```python
#!/usr/bin/env python3
"""
Index all chapters from book-source/docs/docs/*.md
into Qdrant Cloud.
"""

import os
import re
from pathlib import Path
from sentence_transformers import SentenceTransformer
from qdrant_client import QdrantClient
from qdrant_client.models import PointStruct, VectorParams, Distance

# Configuration
QDRANT_URL = os.getenv("QDRANT_URL")
QDRANT_API_KEY = os.getenv("QDRANT_API_KEY")
DOCS_DIR = Path(__file__).parent.parent.parent / "book-source" / "docs" / "docs"
COLLECTION_NAME = "book_passages"
EMBEDDING_DIMENSION = 1536

def extract_sections(markdown_file):
    """Extract chapter, modules, and sections from markdown."""
    with open(markdown_file, 'r') as f:
        content = f.read()

    # Extract chapter name (from H1)
    chapter_match = re.search(r'^# (.+)$', content, re.MULTILINE)
    chapter = chapter_match.group(1) if chapter_match else "Unknown Chapter"

    # Split by H3 sections
    sections = re.split(r'^### ', content, flags=re.MULTILINE)

    result = []
    for section in sections[1:]:  # Skip preamble
        lines = section.split('\n')
        section_title = lines[0].strip()
        section_text = '\n'.join(lines[1:]).strip()

        result.append({
            'chapter': chapter,
            'module': extract_module(section_text),  # Extract H2
            'section': slugify(section_title),
            'text': section_text
        })

    return result

def extract_module(text):
    """Extract module name from section text."""
    match = re.search(r'^## (.+)$', text, re.MULTILINE)
    return match.group(1) if match else "Unknown Module"

def slugify(text):
    """Convert text to URL-safe slug."""
    return re.sub(r'[^a-z0-9-]', '-', text.lower()).strip('-')

def index_chapters():
    """Index all chapters."""
    # Initialize clients
    model = SentenceTransformer('all-minilm-l6-v2')
    client = QdrantClient(url=QDRANT_URL, api_key=QDRANT_API_KEY)

    # Create or recreate collection
    try:
        client.recreate_collection(
            collection_name=COLLECTION_NAME,
            vectors_config=VectorParams(
                size=EMBEDDING_DIMENSION,
                distance=Distance.COSINE
            )
        )
    except Exception as e:
        print(f"Note: Collection may already exist: {e}")

    # Index all markdown files
    points = []
    point_id = 1

    for md_file in DOCS_DIR.glob('*.md'):
        print(f"Indexing {md_file.name}...")

        for section in extract_sections(md_file):
            # Generate embedding
            embedding = model.encode(
                section['text'],
                convert_to_numpy=True
            ).tolist()

            # Create Qdrant point
            point = PointStruct(
                id=point_id,
                vector=embedding,
                payload={
                    'chapter': section['chapter'],
                    'module': section['module'],
                    'section': section['section'],
                    'text': section['text'],
                    'tokens': len(section['text'].split())
                }
            )
            points.append(point)
            point_id += 1

    # Upload to Qdrant
    client.upsert(
        collection_name=COLLECTION_NAME,
        points=points,
        batch_size=100
    )

    print(f"Indexed {len(points)} passages.")

if __name__ == '__main__':
    index_chapters()
```

**Run**: `python backend/scripts/index_chapters.py`

#### Scheduled Re-indexing (Future)

- Trigger on book chapter updates
- Use GitHub Actions workflow to detect changes in `/docs/docs/`
- Re-index changed chapters only (incremental)

---

## Monitoring & Observability

### Logging Strategy

**Logger Configuration** (`src/middleware/logging.py`):

```python
import logging
import json
import re
from datetime import datetime

class StructuredLogger:
    def __init__(self):
        self.logger = logging.getLogger('rag-chatbot')
        handler = logging.StreamHandler()
        formatter = logging.JSONFormatter()
        handler.setFormatter(formatter)
        self.logger.addHandler(handler)

    def log_request(self, method, path, headers):
        """Log incoming request (redact sensitive data)."""
        self.logger.info({
            'event': 'request',
            'method': method,
            'path': path,
            'timestamp': datetime.utcnow().isoformat(),
            'user_agent': headers.get('user-agent')
        })

    def log_retrieval(self, question, num_results, latency_ms):
        """Log retrieval event."""
        self.logger.info({
            'event': 'retrieval',
            'latency_ms': latency_ms,
            'num_results': num_results,
            'timestamp': datetime.utcnow().isoformat()
        })

    def log_error(self, error_type, message):
        """Log error (redact user data)."""
        # Redact question text
        message = re.sub(r'question=".+?"', 'question="[REDACTED]"', message)
        self.logger.error({
            'event': 'error',
            'type': error_type,
            'message': message,
            'timestamp': datetime.utcnow().isoformat()
        })

logger = StructuredLogger()
```

### Metrics to Track

| Metric | Target | Tool |
|--------|--------|------|
| Request latency (p95) | ‚â§ 3s | Custom instrumentation |
| Retrieval latency | ‚â§ 200ms | Qdrant API metrics |
| Generation latency | ‚â§ 1500ms | Groq API metrics |
| Error rate | < 1% | Application logs |
| Uptime | ‚â• 99% | Render.com dashboard |
| Concurrent users | TBD | Rate limit tracking |

### Observability Stack

- **Logs**: Structured JSON to stdout ‚Üí Render.com log viewer
- **Metrics**: FastAPI `/metrics` endpoint (future: Prometheus integration)
- **Traces**: FastAPI debug mode (development); OpenTelemetry (production, future)
- **Alerts**: Render.com error notifications; manual dashboard checks

---

## Risk Mitigation

### Key Risks & Mitigations

| Risk | Impact | Mitigation |
|------|--------|-----------|
| **Qdrant free tier limit exceeded** | Chatbot becomes unavailable | Monitor vector count; upgrade tier if needed before limit |
| **Groq API rate limit** | 429 Too Many Requests errors | Implement request queuing; show user-friendly message; recommend retry |
| **Cold start latency** | First request slow | Pre-warm endpoint via scheduled health checks; document to users |
| **LLM hallucination** | Incorrect answers | Relevance threshold (0.85) filters weak matches; confidence scoring shows uncertainty |
| **Privacy violation** | Questions stored without consent | Default: no logging; questions only logged with explicit opt-in |
| **CORS misconfiguration** | Frontend blocked | Whitelist Vercel domain + localhost for dev; test with curl |
| **API key leak** | Security breach | `.env` file (never committed); Render environment variables; redact from logs |

---

## Implementation Quickstart

### For Backend Developers

**1. Clone and Setup**

```bash
cd backend
python -m venv venv
source venv/bin/activate  # or `venv\Scripts\activate` on Windows
pip install -r requirements.txt
cp .env.example .env
```

**2. Configure Environment**

Edit `.env`:

```
QDRANT_URL=https://<your-cluster>.qdrant.io
QDRANT_API_KEY=<your-key>
GROQ_API_KEY=<your-key>
ALLOWED_ORIGINS=http://localhost:3000,https://book.yoursite.com
ENVIRONMENT=development
LOG_LEVEL=debug
```

**3. Run Server**

```bash
uvicorn backend.main:app --reload
```

Visit: `http://localhost:8000/api/docs`

**4. Index Book Content**

```bash
python backend/scripts/index_chapters.py
```

**5. Run Tests**

```bash
pytest tests/ -v --cov=src
```

### For Frontend Developers

**1. Add ChatBot Component**

```bash
cd book-source
npm install
```

**2. Create ChatBot Component**

Create `src/components/ChatBot.tsx` (see Frontend Integration section)

**3. Add Routes**

Create `src/pages/chatbot.tsx` (see Frontend Integration section)

**4. Run Dev Server**

```bash
npm run start
```

Visit: `http://localhost:3000/chatbot`

**5. Run Tests**

```bash
npm test
```

---

## Acceptance Criteria (Planning Phase - UPDATED)

‚úÖ **Architecture updated**: Floating widget in bottom-left corner (not dedicated page)
‚úÖ **Data model defined**: Document, Question, Answer entities
‚úÖ **API contracts specified**: OpenAPI-compatible `/api/chat` endpoint
‚úÖ **Backend services designed**: 5 core services (embedding, retrieval, generation, confidence, error handling)
‚úÖ **Frontend component designed**: Floating React/TypeScript widget with:
   - Toggle button (üí¨) always visible in bottom-left corner
   - Collapsible card widget (380px width on desktop, responsive on mobile)
   - Auto-open on text selection (optional)
   - Sticky positioning (stays fixed during scroll)
   - Mobile-responsive (‚â§640px width)
‚úÖ **Docusaurus integration**: Global Root wrapper component for seamless embedding
‚úÖ **Deployment strategy documented**: Render + Vercel
‚úÖ **Monitoring & observability planned**: Structured logging, metrics, alerts
‚úÖ **Risk mitigation strategies identified**: Rate limiting, cold starts, fallbacks
‚úÖ **Constitution compliance verified**: All 6 principles met (content-first, reusable, personalized, accessible, secure, RAG quality)
‚úÖ **Quality gates defined**: Code quality (type hints, 80% coverage), testing, performance (p95 ‚â§ 3s), security (no secrets in logs)
‚úÖ **Architectural decision**: Floating widget vs. dedicated page (APPROVED)

---

**Plan Version**: 1.0
**Status**: Complete
**Ready for**: Task Breakdown (`/sp.tasks`)
